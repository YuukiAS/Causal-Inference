---
title: 'Double Robust version of ELW: Simulation-2'
author: 'Mingcheng Hu'
---

# Set Up Environment

```{r include = FALSE}
library(MASS)
library(knitr)
library(knitrProgressBar)
library(tidyverse)

set.seed(1234)
options(digits = 3)
```

```{r label = "Constants"}
nrep = 2000
nrep_small = 100

N_large = 100000
N_normal = 20000
N_small = 5000
```



# Implementing Estimators

```{r label = "Utility Functions"}
expit = function(x) 1 / (1 + exp(-x))
logit = function(p) log(p / (1 - p))
```

## Non-DR Estimators

```{r}
#' Missing model: Any
#' Imputation model: Any
IPW = function(y, D, prop) {
    N = length(y)
    return(sum(D / prop * y) / N)
}
```


```{r}
# Ref Kang, Joseph D. Y., and Joseph L. Schafer. “Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data.” Statistical Science, vol. 22, no. 4, Nov. 2007. DOI.org (Crossref), https://doi.org/10.1214/07-STS227.


# Ref (3), Table 1
# Reweighting to resemble the full population
#' Missing model: Any
#' Imputation model: Any
SIPW_POP = function(y, D, prop) { # prop is estimated using missing model
    return(sum(D / prop * y) / sum(D / prop)) # modified IPW
}

# Ref (4), Table 1
# Reweighting to resemble the nonrespondents, return the estimate of full population
#' Missing model: Any
#' Imputation model: Any
SIPW_NR = function(y, D, prop) {
    r1 = mean(D) # proportion of respondents
    r0 = mean(1 - D)
    # * g = 1-e(x) in A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights
    mu_NR = sum(D / prop * (1 - prop) * y) / sum(D / prop * (1 - prop))  # * y0 in A Framework for Causal Inference...
    return(r1 * mean(y[D == 1]) + r0 * mu_NR)
}

# Ref (6), Table 2
# We use the matrixed version below for faster computation
#' Missing model: Any
#' Imputation model: Any
Strat_Pi = function(y, D, prop, S = 5) {
    N = length(y)
    si = cut(prop, breaks = S, labels = 1:S) # each individual's stratum
    ci = matrix(NA, nrow = N, ncol = S) # indicator matrix
    for (s in 1:S) {
        for (i in 1:N) {
            ci[i, s] = (si[i] == s)
        }
    }
    strata = rep(NA, S)
    for (s in 1:S) {
        strata[s] = (sum(ci[, s]) / N) * (sum(ci[, s] * D * y) / sum(sum(ci[, s] * D)))
    }
    return(sum(strata))
}

Strat_Pi_matrix = function(y, D, prop, S = 5) {
    N = length(y)
    si = cut(prop, breaks = S, labels = 1:S)
    ci = sapply(1:S, function(s) si == s)
    strata = colSums(ci) / N * colSums(ci * D * y) / colSums(ci * D)
    return(sum(strata))
}

# Ref (7), Table 3
# * See comment for demystifing paper why it works well
#' Missing model: Any
#' Imputation model: Linear
OLS = function(y, D, x, impute_model_formula) {
    data = data.frame(y = y, D = D, x = x)
    impute_model = lm(impute_model_formula, data = subset(data, D == 1))
    return(mean(impute_model, newdata = data))
}

# Ref Table 4
#' Missing model: Any
#' Imputation model: Any
Strat_Pi_y = function(y, D, prop, y_imputed, S = 5) {
    si_pi = cut(prop, breaks = S, labels = 1:S)
    si_y = cut(y_imputed, breaks = S, labels = 1:S)

    table_si = table(si_pi, si_y)
    cell_means = matrix(NA, nrow = S, ncol = S)
    cell_n = matrix(NA, nrow = S, ncol = S)

    for (i in 1:S) {
        for (j in 1:S) {
            # All individuals in the cell
            cell_indices = which(si_pi == i & si_y == j)

            cell_n[i, j] = length(cell_indices)
            if (length(cell_indices) > 0) {
                t1_indices_cell = cell_indices[D[cell_indices] == 1]

                if (length(t1_indices_cell) > 0) {
                    cell_means[i, j] = mean(y[t1_indices_cell])
                }
            }
        }
    }

    # Handle offending cells (cells that only has non-respondents)
    for (i in 1:S) {
        for (j in 1:S) {
            # If the cell mean is NA (offending cell)
            if (is.na(cell_means[i, j]) && cell_n[i, j] > 0) {
                # Here we use imputation model assuming only row and column effects (no interaction)
                row_effect = mean(cell_means[i, ], na.rm = TRUE) # Row-wise mean
                col_effect = mean(cell_means[, j], na.rm = TRUE) # Column-wise mean
                overall_mean = mean(cell_means, na.rm = TRUE) # Overall mean
                cell_means[i, j] = row_effect + col_effect - overall_mean
            }
        }
    }

    cell_means_vector = as.vector(cell_means)
    cell_n_vector = as.vector(cell_n)
    return(weighted.mean(cell_means_vector, cell_n_vector, na.rm = TRUE))
}
```

```{r}
# Ref Zong, Xianpeng, et al. Improved Horvitz-Thompson Estimator in Survey Sampling. arXiv:1804.04255, arXiv, 11 Apr. 2018. arXiv.org, http://arxiv.org/abs/1804.04255.
# Ref Section 3
# * Truncated estimator (down-weighting)
#' Missing model: Any
#' Imputation model: Any
IPW.zzz = function(y, D, prop) {
    N = length(y)
    index = 1:N
    tmp = sort(prop)
    i = max(which(tmp < 1 / (index + 1))) # * hard-threshold based on rank
    crit = tmp[i]

    prop.star = prop
    prop.star[which(prop <= crit)] = crit
    return(sum(D * y / prop.star) / N)
}

# Ref Crump R. K., Hotz V. J., Imbens G. W., & Mitnik O. A. (2009). Dealing with limited overlap in estimation of average treatment effects. http://doi.org/10.1093/biomet/asn055
# * Trimmied estimator (dicarding)
#' Missing model: Any
#' Imputation model: Any
IPW.chim = function(y, D, prop) {
    tempfun = function(a) {
        u = prop * (1 - prop)
        v = a * (1 - a)
        z = (u >= v)
        a + (2 * v * sum(z / u) > sum(z))
    }
    a = optimize(tempfun, lower = 1e-5, upper = 0.5, maximum = FALSE)$minimum

    N = length(y)
    y = rep(y, D)
    pw = rep(prop, D)

    ind = (pw > a & pw < 1 - a)
    y.star = y[ind]
    pw.star = pw[ind]
    return(sum(y.star / pw.star) / sum(1 / pw.star))
}

# Ref Ma, Xinwei, and Jingshen Wang. Robust Inference Using Inverse Probability Weighting. arXiv:1810.11397, arXiv, 24 May 2019. arXiv.org, http://arxiv.org/abs/1810.11397.
# Ref Section 3.2
# * Trimmed estimator
#' Missing model: Any
#' Imputation model: Any
IPW.trim = function(y, D, prop) {
    N = length(D)
    n = sum(D)

    # Ref Theorem2
    fun1 = function(t, s) {
        2 * n * t^s * mean(prop <= t) - 1
    }
    # define Trimming threshold
    bn1 = uniroot(fun1, c(0, 1), s = 1)$root # moderate trimming
    bn2 = uniroot(fun1, c(0, 1), s = 2)$root # heavy trimming

    # define Bandwidth sequence for local polynomial regression
    # Ref Supplementary Material II.1
    fun2 = function(t) {
        n * t^5 * mean(prop <= t) - 1
    }
    hn = uniroot(fun2, c(0, 1))$root

    y1 = rep(y, D) # observed Y
    pw = rep(prop, D)
    xx = cbind(rep(1, n), pw)
    ww = (pw < hn) # We use subsample in region (0,hn) for local polynomial regression
    xx1 = cbind(ww, ww * pw)
    xx.all = cbind(rep(1, N), prop)

    # Ref Algorithm1, Step1
    beta = ginv(t(xx1) %*% xx) %*% t(xx1) %*% y1

    # define Bias correction terms
    # Ref Algorithm1, Step2
    Bnb1 = -mean((xx.all %*% beta) * (prop < bn1))
    ipw.trim1 = mean(y1 * (pw >= bn1) / pw) * n / N
    ipw.bc1 = ipw.trim1 - Bnb1

    Bnb2 = -mean((xx.all %*% beta) * (prop < bn2))
    ipw.trim2 = mean(y1 * (pw >= bn2) / pw) * n / N
    ipw.bc2 = ipw.trim2 - Bnb2

    return(c(ipw.bc1, ipw.bc2)) # different tuning parameters s = 1 and s = 2
}
```

```{r}
# Ref Matsouaka, Roland A., and Yunji Zhou. A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights: The Role of Overlap Weights. arXiv:2011.01388, arXiv, 24 Oct. 2022. arXiv.org, http://arxiv.org/abs/2011.01388.
# Ref 3.2.2


# * We use stablized version, similar to the one in demystifing paper
#' Missing model: Any
#' Imputation model: Any
OW = function(y, D, prop) {
    g = prop * (1 - prop)
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}

#' Missing model: Any
#' Imputation model: Any
MW = function(y, D, prop) {
    g = min(prop, 1 - prop)
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}

#' Missing model: Any
#' Imputation model: Any
EW = function(y, D, prop) {
    g = -(prop * log(prop) + (1 - prop) * log(1 - prop))
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}

#' Missing model: Any
#' Imputation model: Any
# * We follow the simulation in 4.5.2 and set nu = 11 (smaller SE) or 81 (smaller bias)
BW = function(y, D, prop, nu) {
    if (nu < 2) {
        stop("nu must be greater than or equal to 2")
    }

    g = (prop * (1 - prop)) ^ nu
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}
```


```{r}
# Ref Liu, Yukun. “Biased-Sample Empirical Likelihood Weighting for Missing Data Problems: An Alternative to Inverse Probability Weighting.” Statistical Methodology, vol. 85, no. 1, 2023.
#' Missing model: Any
#' Imputation model: Any
ELW = function(y, D, prop, tol = sqrt(.Machine$double.eps)) { # tol is the tolerance level
    N = length(y)
    n = sum(D)
    y1 = y[D == 1]
    prop1 = prop[D == 1] # propensity score of respondents
    low = min(prop1)
    xi = n / N + (1 - n / N) * prop1
    up = min(xi) - tol

    fun_alpha = function(alpha) {
        sum((prop1 - alpha) / (xi - alpha))
    }
    alpha = uniroot(fun_alpha, interval = c(low, up), tol = tol)$root
    lambda = (N / n - 1) / (1 - alpha)
    tmp = 1 + lambda * (prop1 - alpha)
    prob.elw = 1 / (n * tmp)

    return(sum(y1 * prob.elw))
}
```

## DR Estimators

```{r}
# * This is also BC (bias corrected)-OLS in demystifing paper, Table 5
# * This is also mu_DR(pi, m_REG) in the comment for demystifing paper
#' Missing model: Any
#' Imputation model: Any
AIPW = function(y, D, prop, y_imputed) {
    N = length(y)
    return(mean(y_imputed) + sum(D / prop * (y - y_imputed)) / N)
}

# B-DR in comment for demystifing paper
#' Missing model: Any
#' Imputation model: Any
AIPW_Stablized = function(y, D, prop, y_imputed) {
    N = length(y)
    return(mean(y_imputed) + sum(D / prop * (y - y_imputed)) / sum(D / prop))
}
```

```{r}
# Ref Kang, Joseph D. Y., and Joseph L. Schafer. “Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data.” Statistical Science, vol. 22, no. 4, Nov. 2007. DOI.org (Crossref), https://doi.org/10.1214/07-STS227.

# Ref (10), Table 6
#' Missing model: Any
#' Imputation model: Linear
WLS = function(y, D, x, prop, impute_model_formula) {
    data = data.frame(y = y, D = D, x = x, prop = prop)
    impute_model = lm(impute_model_formula, data = subset(data, D == 1), weights = 1 / prop)
    return(mean(impute_model, newdata = data))
}

# * Coarsening eta into five categories and creating dummy indicators
#' Missing model: Any
#' Imputation model: Linear
Pi_Cov_1 = function(y, D, x, prop, impute_model_formula) {
    eta = expit(prop)
    eta_cat = cut(eta, breaks = 5, labels = 1:5)
    eta_dummy = model.matrix(~ eta_cat - 1)
    data = data.frame(y = y, D = D, x = x, prop = prop, eta_dummy)
    impute_model_formula_extended = paste(impute_model_formula, "+ eta_dummy")
    impute_model = lm(impute_model_formula_extended, data = subset(data, D == 1))
    return(mean(impute_model, newdata = data))
}

# Ref Table 8
# * This is also mu_DR(pi, m_EXTREG) in the comment for demystifing paper
#' Missing model: Any
#' Imputation model: Linear
Pi_Cov_2 = function(y, D, x, prop, impute_model_formula) {
    prop_inv = 1 / prop
    impute_model_formula_extended = paste(impute_model_formula, "+ I(prop_inv)")
    data = data.frame(y = y, D = D, x = x, prop = prop, prop_inv)
    impute_model = lm(impute_model_formula_extended, data = subset(data, D == 1))
    return(mean(impute_model, newdata = data))
}
```


```{r}
library(tmle) # Tested on version 1.3.0, not working on latest version
# Ref Gruber, Susan, and Mark J. Van Der Laan. “Tmle : An R Package for Targeted Maximum Likelihood Estimation.” Journal of Statistical Software, vol. 51, no. 13, 2012. DOI.org (Crossref), https://doi.org/10.18637/jss.v051.i13.
# Ref Page 17 (Mis-specified imputation model)
#' Missing model: Any
#' Imputation model: Any
TMLE = function(y, D, x, prop_model_formula, impute_model_formula) {
    result.EY1 = tmle(Y,
        A = rep(1, n), x,
        Qform = impute_model_formula, g.Deltaform = prop_model_formula, Delta = D
    )
    return(result.EY1$estimates$EY1$psi)
}
```

```{r}
# Ref Vermeulen, Karel, and Stijn Vansteelandt. “Bias-Reduced Doubly Robust Estimation.” Journal of the American Statistical Association, vol. 110, no. 511, July 2015, pp. 1024–36. DOI.org (Crossref), https://doi.org/10.1080/01621459.2014.958155.
# Ref Section 3.3, Appendix J
# * Missing model: Logistic
# * Conditional mean model: Linear/Logistic
DR_BR = function(y, D, x, type) {
    n = length(D)
    xx = cbind(rep(1, n), x)

    # define Functions to obtain an estimator of gamma
    # Linear conditional mean model
    min.Uint.linear = function(gamma) {
        -mean((-D * exp(-gamma %*% t(xx))) + (-(1 - D) * (gamma %*% t(xx))))
    }

    # Logistic conditional mean model
    min.Uint.logistic = function(gamma) {
        -mean(((-R * exp(-gamma %*% t(xx))) + (-(1 - D) * (gamma %*% t(xx)))) *
            as.vector(expit(init.beta %*% t(xx)) * (1 - expit(init.beta %*% t(xx)))))
    }

    # This is exactly AIPW
    U = function(D, y, X, gamma, beta) {
        (D / expit(gamma %*% t(X)) * (y - beta %*% t(X)) + beta %*% t(X))
    }

    init.gamma = coef(glm(D ~ x, family = "binomial"))
    if (type == "linear") {
        sol = nlm(min.Uint.linear, init.gamma) # nonlinear minimization
    } else if (type == "logistic") {
        sol = nlm(min.Uint.logistic, init.gamma)
    } else {
        stop("Invalid type")
    }
    # define Parameters of missing model
    gamma.BR = sol$estimate

    weight = as.vector(1 / exp(gamma.BR %*% t(xx)))
    # define Parameter of imputation model
    beta.BR = coef(lm(y ~ -1 + xx, subset = (D == 1), weights = weight)) # only consider respondents

    # define Estimate of mean outcome
    mn.y = mean(U(D, y, xx, gamma.BR, beta.BR))
    return(mn.y)
}
```

```{r}
# * Proposed estimator
ELW_DR = function(y, D, prop, y_imputed, eps = sqrt(.Machine$double.eps)) { # eps is the tolerance level
    N = length(y)
    n = sum(D)

    # Only consider respondents
    y1 = y[D == 1]
    # y1_imputed = y_imputed[D == 1]
    prop1 = prop[D == 1]

    low = min(prop1)
    xi = n / N + (1 - n / N) * prop1
    up = min(xi) - eps

    fun_alpha = function(alpha) {
        sum((prop1 - alpha) / (xi - alpha))
    }
    alpha = uniroot(fun_alpha, interval = c(low, up), tol = eps)$root
    lambda = (N / n - 1) / (1 - alpha)
    tmp = 1 + lambda * (prop1 - alpha)
    prob.elw1 = 1 / (n * tmp)
    prob.elw = rep(0, N)
    prob.elw[D == 1] = prob.elw1

    # * Note here we can use prob.elw * y as prob.elw is only nonzero for respondents
    return(sum(prob.elw * y) + mean(y_imputed) - sum(prob.elw * y_imputed))
}
```

```{r}
# Ref Robins, James, et al. “Comment: Performance of Double-Robust Estimators When ‘Inverse Probability’ Weights Are Highly Variable.” Statistical Science, vol. 22, no. 4, Nov. 2007. DOI.org (Crossref), https://doi.org/10.1214/07-STS227D.

# AIPW estimator can be expressed as mu_DR(pi, m_REG)

# Ref IPW DR Estimator: mu_DR(pi_EXT, m_REG)
#' Missing model: Any
#' Imputation model: Linear
DR_Bounded_IPW = function(y, D, x, prop_model_formula, impute_model_formula) {
    data = data.frame(y = y, D = D, x = x)
    mu_OLS = OLS(y, D, x, impute_model_formula)
    y_imputed = lm(impute_model_formula, data = subset(data, D == 1))$fitted.values
    h = y_imputed - mu_OLS
    prop_model_formula_extended = paste(prop_model_formula, "+ h")
    data = data.frame(y = y, D = D, x = x, h = h)
    prop = glm(prop_model_formula_extended, family = binomial)$fitted.values
    return(AIPW_Stablized(y, D, prop, y_imputed))
}

# Ref Regression DR Estimator1: mu_DR(pi, m_EXTREG)
# Already implemented above as Pi_Cov_2

# Ref Regression DR Estimator2: mu_DR(pi, m_WLS)
# Already implemented above as WLS

# Ref Regression DR Estimator3: mu_DR(pi, m_DR_IPW_NR)
# * Adding the covariate pi instead of adding 1/pi doesn't induce model extrapolation problems
#' Missing model: Any
#' Imputation model: Linear
DR_Bounded_Regression = function(y, D, x, prop, impute_model_formula) {
    impute_model_formula_extended = paste(impute_model_formula, "+ prop")
    data = data.frame(y = y, D = D, x = x, prop = prop)
    impute_model = lm(impute_model_formula_extended, data = subset(data, D == 1))
    return(mean(impute_model, newdata = data))
}
```

```{r}
library(CBPS)
library(nleqslv)
# Ref Imai, Kosuke, and Marc Ratkovic. “Covariate Balancing Propensity Score.” Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 76, no. 1, Jan. 2014, pp. 243–63. DOI.org (Crossref), https://doi.org/10.1111/rssb.12027.

CBPS = function(y, D, x, prop_model_formula) {
    # Follow the code in https://github.com/kosukeimai/CBPS/blob/master/R/CBPSMain.R
    CBPS.fit = CBPS(prop_model_formula, ATT = 0)
    return(mean(D*y/CBPS.fit$fitted.values))
}

# Ref Cao, Weihua, et al. “Improving Efficiency and Robustness of the Doubly Robust Estimator for a Population Mean with Incomplete Data.” Biometrika, vol. 96, no. 3, Sept. 2009, pp. 723–34. DOI.org (Crossref), https://doi.org/10.1093/biomet/asp033.
# Ref Equation 16
#' Missing model: Logistic
#' Imputation model: Linear
CTD = function(y, D, x, prop_model_formula, impute_model_formula) {
    data = data.frame(y = y, D = D, x)  # Don't put x=x to keep separate columns
    prop = glm(prop_model_formula, family = binomial, data=data)$fitted.values
    impute_model = lm(impute_model_formula, data = subset(data, D == 1))
    y_imputed = predict(impute_model, newdata = data)

    # p+q dimensions in total
    x_impute = model.matrix(impute_model_formula, data = data.frame(y, x))
    p = ncol(x_impute)
    m_beta = x_impute  # derivative of imputation model w.r.t beta
    x_prop = model.matrix(prop_model_formula, data = data.frame(D, x))
    q = ncol(x_prop)
    pi_theta = prop * (1 - prop) * x_prop    # derivative of missing model w.r.t theta

    term1 = D / prop   # N*1
    term2 = (1 - prop) / prop  # N*1
    term3.1 = m_beta  # N*p
    term3.2 = pi_theta / (1 - prop)  # N*q

    estimating_equations <- function(params) {
        beta <- params[1:p]
        c <- params[(p + 1):(p + q)]

        y_imputed_beta = m_beta %*% beta
        term4 = y - y_imputed_beta - (pi_theta %*% c) / (1 - prop)  # N*1

        eq_beta <- colSums(sweep(term3.1, 1, term1 * term2 * term4, `*`))
        eq_c <- colSums(sweep(term3.2, 1, term1 * term2 * term4, `*`))
        # eq_beta <- colSums(term1 * term2 * term3.1 * term4)
        # eq_c <- colSums(term1 * term2 * term3.2 * term4)
        equations = c(eq_beta, eq_c)

        return(equations)
    }

    beta_init = coef(lm(impute_model_formula, data = subset(data, D == 1)))
    init_params <- c(beta_init, rep(0, q))
    solution <- nleqslv(init_params, estimating_equations)

    beta_final = solution$x[1:p]
    c = solution$x[(p + 1):(p + q)]

    y_imputed_final = m_beta %*% beta_final

    # print(beta_init)
    # print(beta_final)
    return(AIPW(y, D, prop, y_imputed_final))
}

# Ref Zhang, Min, and Baqun Zhang. “A Stable and More Efficient Doubly Robust Estimator.” Statistica Sinica, 2022. DOI.org (Crossref), https://doi.org/10.5705/ss.202019.0265.
# * We use the setting in Section 4, where there are three options of hn
# * For simplicity, we use Gassian as the Nadaraya-Watson kernel
#' Missing model: Any
#' Imputation model: Any
ZZ = function(y, D, prop, y_imputed, hn_power, progress_bar = FALSE) { # h_n is the bandwidth
    if (!hn_power %in% c(-1/3, -1/4, -1/5)) {
        stop("hn_power must be one of -1/3, -1/4, -1/5")
    }

    # * We use Gaussian kernel for simplicity
    kernel_function <- function(u) {
        exp(-0.5 * u^2) / sqrt(2 * pi)  
    }

    N = length(y)
    h_n = N^hn_power

    numerator_sum <- 0
    denominator_sum <- 0
  
    if(progress_bar) {
        pb = progress_estimated(N)
    }
    for (i in 1:N) {
        if (progress_bar) {
            update_progress(pb)
        }
        numerator_i <- 0
        denominator_i <- 0
        for (j in 1:N) {
            kernel_weight <- kernel_function((prop[j] - prop[i]) / h_n)
            numerator_i <- numerator_i + D[j] * (y[j] - y_imputed[j]) * kernel_weight
            denominator_i <- denominator_i + D[j] * kernel_weight
        }
        numerator_sum <- numerator_sum + (numerator_i / denominator_i) + y_imputed[i]
    }

    # population mean
    mu <- numerator_sum / N
    return(mu)
}
```


# Various Data Generation Mechanisms with Extreme with Extreme Weights

For the incorrect models, we follow the convention in *A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights* and omit the first covariate.

In order to make use of the estimators defined above, we need to supply `prop`, `y_imputed`, `prop_model_formula` and `impute_model_formula` for each data generation mechanism. We will provide a list through `fitted_model`.


```{r}
Fitted.model0 = setRefClass("Fitted.model0",
    fields = list(
        y = "numeric",  # length is N
        D = "numeric",
        data = "data.frame"
    ),
    methods = list(
        get_x = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_prop_model_formula.correct = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_prop_model_formula.incorrect = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_impute_model_formula.correct = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_impute_model_formula.incorrect = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_prop.correct = function() {
            prop = glm(get_prop_model_formula.correct(), family = binomial, data = data)$fitted.values
            return(prop)
        },
        get_prop.incorrect = function() {
            prop = glm(get_prop_model_formula.incorrect(), family = binomial, data = data)$fitted.values
            return(prop)
        },
        get_y_imputed.correct = function() {
            impute_model = lm(get_impute_model_formula.correct(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function() {
            impute_model = lm(get_impute_model_formula.incorrect(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```

## Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data & Comment: Performance of Double-Robust Estimators When "Inverse Probability" Weights Are Highly Variable

The simulation 1 is proposed in demystifing paper. This is a standard scenario and has been widely used in a variety of related papers.

```{r}
#' Model mis-speification: Wrong covariates
data.gen1 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    y = 210 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4 + epsilon
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)

    # wrong covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    data = list(
        y = y, D = D,
        z1 = z1, z2 = z2, z3 = z3, z4 = z4,
        x1 = x1, x2 = x2, x3 = x3, x4 = x4
    )
    return(list(data = data, prop = prop))
}

# Example:  `fitted.model1 = do.call(Fitted.model1$new, data)`
Fitted.model1 = setRefClass("Fitted.model1",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        }
    )
)
```


The simulation 2 is a modification of the original one. We change the setting from $D=1$ being the respondents to $D=0$ being the respondents. This is proposed in the comment paper, designed so that OLS estimator no longer performs better than DR estimators.


```{r}
#' Model mis-speification: Wrong covariates
data.gen2 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    y = 210 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4 + epsilon
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)
    # * We analyze the data in which Y is observed only when D = 0
    # This should lead to weights of opposite signs
    D = 1 - D

    # covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    return(
        list(
            y = y,
            D = D,
            z1 = z1, z2 = z2, z3 = z3, z4 = z4,
            x1 = x1, x2 = x2, x3 = x3, x4 = x4
        ),
        prop = 1 - prop
    )
}

# Same as Fitted.model1
Fitted.model2 = setRefClass("Fitted.model2",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        }
    )
)
```



## A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights

**We shall not address the original treatment effect estimation problem. Instead, we take the data as missing data and wish to estimate the average earnings of the treated.**

### Illustrative Example

This is a modified example of the original one. We set $Y=Y_1$ so that the response no longer depends on $D$.

```{r}
alpha_A.gen3 = c(-2.8, 0.2, 0.8) # p = 20% (extreme weight)
alpha_B.gen3 = c(-1.6, 0.45, 0.6) # p = 50%
alpha_C.gen3 = c(0.2, 0.8, 0.2) # p = 80%

#' Model mis-speification: Ignore covariates (Set to first one)
data.gen3 = function(N, alpha = alpha_A.gen1) {
    x1 = rnorm(N, 2, 2)
    x2 = rnorm(N, 1, 1)

    prop = 1 / (1 + exp(-(alpha[1] + alpha[2] * x1 + alpha[3] * x2)))
    D = rbinom(N, 1, prop)

    epsilon1 = rnorm(N, 0, 2)
    # epsilon2 = rnorm(N, 0, 2)
    # y = ifelse(D == 1, 2 + x1 + x2 + 2*x1^2 + 0.5*x2^2 + epsilon1, x1 + x2 + epsilon2)
    y = 2 + x1 + x2 + 2 * x1^2 + 0.5 * x2^2 + epsilon1

    return(list(data = data.frame(y, D, x1, x2), prop = prop))
}

Fitted.model3 = setRefClass("Fitted.model3",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        data = "data.frame"       
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x2"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + I(x1^2) + I(x2^2)"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + I(x2^2)"))
        }
    )
)
```

### First Simulation

In this case we choose the homogeneous treatment effect $\Delta=3$. Note that as we modify the setting, this will be incorporated as the intercept of the conditional mean model.

```{r}
alpha_A.gen4 = c(-0.5, 0.3, 0.4, 0.4, 0.4) # good overlap
alpha_B.gen4 = c(-1, 0.6, 0.8, 0.8, 0.8) # moderate overlap
alpha_C.gen4 = c(-1.5, 0.9, 1.2, 1, 2, 1.2) # poor overlap

#' Model mis-speification: Ignore covariates (Set to first one)
data.gen4 = function(N, alpha = alpha_C.gen4) {
    x4 = rbinom(N, 1, 0.5)
    x3 = rbinom(N, 1, 0.4 + 0.2 * x4)
    mu_matrix = cbind(x4 - x3 + 0.5 * x3 * x4, -x4 + x3 + x3 * x4)
    Sigma_elements = cbind(2 - x3, 0.25 * (1 + x3))
    x1x2 = t(sapply(1:N, function(i) {
        Sigma_matrix = matrix(c(
            Sigma_elements[i, 1], Sigma_elements[i, 2],
            Sigma_elements[i, 2], Sigma_elements[i, 1]
        ), 2, 2)
        mvrnorm(1, mu = mu_matrix[i, ], Sigma = Sigma_matrix)
    }))
    x1 = x1x2[, 1]
    x2 = x1x2[, 2]

    prop = 1 / (1 + exp(-(alpha[1] + alpha[2] * x1 + alpha[3] * x2 + alpha[4] * x3 + alpha[5] * x4)))
    D = rbinom(N, 1, prop)

    epsilon = rnorm(N, 0, 1)
    y = 0.5 + x1 + 0.6 * x2 + 2.2 * x3 + 1.2 * x4 + epsilon
    return(list(data = data.frame(y, D, x1, x2, x3, x4), prop = prop))
}

Fitted.model4 = setRefClass("Fitted.model4",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + x3 + x4"))
        }
    )
)
```

### Second Simulation

In this case we choose the heterogeneous treatment effect $\Delta=prop^2+2prop+1$. The simulation focsues on the impact of proportion of treated participants (prevalence of treatment)

```{r}
alpha.gen5 = c(0.15, 0.3, 0.3, -0.2, -0.25, -0.25)
# low prevalence (around 0.1)
alpha_A.gen5 = c(-2.1, alpha.gen5) # good overlap
alpha_B.gen5 = c(-2.2, 2 * alpha_A.gen5) # moderate overlap
alpha_C.gen5 = c(-2.8, 3 * alpha_A.gen5) # poor overlap

#' Model mis-speification: Ignore covariates (Set to first one)
data.gen5 = function(N, alpha = alpha_C.gen5) {
    mu_matrix = rep(0, 6)
    # unit marginal variance, pairwise covariance of 0.5
    Sigma_matrix = matrix(0.5, nrow = 6, ncol = 6)
    diag(Sigma_matrix) = 1
    x1x2x3x4x5x6 = mvrnorm(N, mu = mu_matrix, Sigma = Sigma_matrix)
    x1 = x1x2x3x4x5x6[, 1]
    x2 = x1x2x3x4x5x6[, 2]
    x3 = x1x2x3x4x5x6[, 3]
    x4 = x1x2x3x4x5x6[, 4]
    x5 = x1x2x3x4x5x6[, 5]
    x6 = x1x2x3x4x5x6[, 6]
    # dichotomize
    x4 = ifelse(x4 > 0, 1, 0)
    x5 = ifelse(x5 > 0, 1, 0)
    x6 = ifelse(x6 > 0, 1, 0)

    prop = 1 / (1 + exp(-(alpha[1] + alpha[2] * x1 + alpha[3] * x2 + alpha[4] * x3 + alpha[5] * x4 + alpha[6] * x5 + alpha[7] * x6)))

    epsilon = rnorm(N, 0, 1.5)
    y = -0.5 * x1 - 0.5 * x2 - 1.5 * x3 + 0.8 * x4 + 0.8 * x5 + x6 + epsilon + prop^2 + 2 * prop + 1
    return(list(data = data.frame(y, D, x1, x2, x3, x4, x5, x6), prop = prop))
}

Fitted.model5 = setRefClass("Fitted.model5",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        x5 = "numeric",
        x6 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = x5,
                           x6 = x6)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4 + x5 + x6") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x2 + x3 + x4 + x5 + x6"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4 + x5 + x6 + prop + I(prop^2)"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + x3 + x4 + x5 + x6 + prop"))
        },
        get_y_imputed.correct = function(prop.correct = TRUE) {
            if (prop.correct == TRUE) {
                prop = get_prop.correct()
            } else {
                prop = get_prop.incorrect()
            }
            impute_model = lm(get_impute_model_formula.correct(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function(prop.correct = FALSE) {
            if (prop.correct == TRUE) {
                prop = get_prop.correct()
            } else {
                prop = get_prop.incorrect()
            }
            impute_model = lm(get_impute_model_formula.incorrect(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```

## A Stable and More Efficient Doubly Robust Estimator

```{r}
#' Model mis-speification: Ignore covariates (Set to last one) as in paper
data.gen6 = function(N) {
    x1 = runif(N, 0, 1)
    x2 = rnorm(N)
    x3 = rbinom(N, 1, 0.3)
    x4 = rlnorm(N, 0, 1) # log-normal

    epsilon = rnorm(N)

    prop = expit(-1 - x1 / 2 + x2 - x3 + x4)
    D = rbinom(N, 1, prop)

    y = 2.5 + x1 / 2 + x2 + x3 + x4 + epsilon

    return(list(y = y, D = D, x1 = x1, x2 = x2, x3 = x3, x4 = x4), prop = prop)
}

Fitted.model6 = setRefClass("Fitted.model6",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + x3 + x4"))
        }
    )
)
```

## Bias-Reduced Doubly Robust Estimation

Scenario 1 is the same as demystifing paper.

### Scenario 2

Incorrect outcome model: $m_0(x)=1+x$
Incorrect propensity model: $\pi_0(x)=expit(\varespilon \cdot x)$

```{r}
#' Model mis-speification: Wrong form of model
data.gen7 = function(N) {
    x = rnorm(N, 0, 1)
    prop = expit(-4 + 1.5 * sqrt(abs(x)) + 0.75 * x + 0.5 * abs(x)^1.5)
    D = rbinom(N, 1, prop)
    y = rnorm(N, x^2, 1)
    return(list(y = y, D = D, x = x), prop = prop)
}

Fitted.model7 = setRefClass("Fitted.model7",
    fields = list(
        x = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x = x)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ sqrt(abs(x)) + x + I(abs(x)^1.5)") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x + I(x^2)"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x"))
        }
    )
)
```

## Improved double-robust estimation in missing data and causal inference models

There are four experiments in total:

1. The first one is just the setting in demystifing paper.
2. The second one is just the setting in *Comment: Performance of Double-Robust Estimators When "Inverse Probability" Weights Are Highly Variable* above, where we exchange the missing indicator.
3. The third one modifies the form of Y, changing it from a linear model to a logistic model.
4. The fourth one is the same as the third one, except we exchange the missing indicator as in (2).

```{r}
#' Model mis-speification: Wrong covariates
data.gen8 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    # y = 210 + 27.4*z1 + 13.7*z2 + 13.7*z3 + 13.7*z4 + epsilon
    y = rbinom(N, 1, expit(-60 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4)) # change from linar to logistic
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)

    # wrong covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    return(
        list(
            y = y,
            D = D,
            z1 = z1, z2 = z2, z3 = z3, z4 = z4,
            x1 = x1, x2 = x2, x3 = x3, x4 = x4
        ),
        prop = prop
    )
}

Fitted.model8 = setRefClass("Fitted.model8",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_y_imputed.correct = function() {
            impute_model = glm(get_impute_model_formula.correct(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function() {
            impute_model = glm(get_impute_model_formula.incorrect(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```

```{r}
#' Model mis-speification: Wrong covariates
data.gen9 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    # y = 210 + 27.4*z1 + 13.7*z2 + 13.7*z3 + 13.7*z4 + epsilon
    y = rbinom(N, 1, expit(-60 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4)) # change from linar to logistic
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)
    # * We analyze the data in which Y is observed only when D = 0
    D = 1 - D

    # wrong covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    return(
        list(
            y = y,
            D = D,
            z1 = z1, z2 = z2, z3 = z3, z4 = z4,
            x1 = x1, x2 = x2, x3 = x3, x4 = x4
        ),
        prop = 1 - prop
    )
}

# Same as Fitted.model8
Fitted.model9 = setRefClass("Fitted.model9",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_y_imputed.correct = function() {
            impute_model = glm(get_impute_model_formula.correct(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function() {
            impute_model = glm(get_impute_model_formula.incorrect(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```


# Simulation and Analysis

+ IPW.zzz: IPW Truncated
+ IPW.chim: IPW Trimmed
+ IPW.trim: IPW Trimmed
+ OW, MW, EW, BW: Propensity Scores Methods
+ DR_BR: Estimate Nuisance Parameters
+ DR_Bounded_IPW, DR_Bounded_Regression: DR Estimator with Boundness Property
+ CBPS: Propensity Scores Methods
+ CTD: Estimate Nuisance Parameters
+ ZZ: Avoid Directly Inverse Weights

```{r}
library(ggplot2)

# Util function
plot_prop_histogram = function(prop, title) {
    # hist(prop, breaks = 20, main = title, xlab = "Propensity Score")

    fig <- ggplot(data.frame(prop), aes(x = prop)) +
        geom_histogram(bins = 20, fill = "blue", color = "black") +
        labs(title = title, x = "Propensity Score", y = "Frequency")

    ggsave(filename = paste0("img/", title, ".png"), plot = fig, width = 8, height = 6)
}
```

```{r}
N = N_normal
data_all = list(
    demystifing1 = data.gen1(N),  # linear imputation
    demystifing2 = data.gen2(N),  # linear imputation
    framework1 = data.gen3(N),  # linear imputation
    framework2 = data.gen4(N),  # linear imputation
    framework3 = data.gen5(N),  # linear imputation
    stable_eff = data.gen6(N),  # linear imputation
    br = data.gen7(N),  # linear imputation
    improved_DR1 = data.gen8(N),  # logistic imputation
    improved_DR2 = data.gen9(N)   # logistic imputation
)
```

```{r}
prop_histogram_all = list(
    demystifing1 = plot_prop_histogram(data_all$demystifing1$prop, "Demystifying 1"),
    demystifing2 = plot_prop_histogram(data_all$demystifing2$prop, "Demystifying 2"),
    framework1 = plot_prop_histogram(data_all$framework1$prop, "Framework 1"),
    framework2 = plot_prop_histogram(data_all$framework2$prop, "Framework 2"),
    framework3 = plot_prop_histogram(data_all$framework3$prop, "Framework 3"),
    stable_eff = plot_prop_histogram(data_all$stable_eff$prop, "Stable and Efficient"),
    br = plot_prop_histogram(data_all$br$prop, "Bias-Reduced"),
    improved_DR1 = plot_prop_histogram(data_all$improved_DR1$prop, "Improved DR 1"),
    improved_DR2 = plot_prop_histogram(data_all$improved_DR2$prop, "Improved DR 2")
)
```

```{r}
fitted_model_all = list(
    demystifing1 = do.call(Fitted.model1$new, data_all$demystifing1),
)
```

```{r}
# IPW = function(y, D, prop)
# SIPW_POP = function(y, D, prop)
# SIPW_NR = function(y, D, prop)
# Strat_Pi_matrix = function(y, D, prop, S = 5) 
# OLS = function(y, D, x, impute_model_formula)  <- linear imputation
# Strat_Pi_y = function(y, D, prop, y_imputed, S = 5) 
# IPW.zzz = function(y, D, prop) 
# IPW.chim = function(y, D, prop)
# IPW.trim = function(y, D, prop)
# OW = function(y, D, prop)
# MW = function(y, D, prop)
# EW = function(y, D, prop)
# BW = function(y, D, prop, nu) -> nu = 11 or 81
# ELW = function(y, D, prop)

#' Imputation model: Linear
Non_DR_Estimators.linear = c(
    IPW,
    SIPW_POP,
    SIPW_NR,
    Strat_Pi_matrix,
    OLS,
    Strat_Pi_y,
    IPW.zzz,
    IPW.chim,
    IPW.trim,
    OW,
    MW,
    EW,
    BW_11 = function(y, D, prop) BW(y, D, prop, nu=11),
    BW_81 = function(y, D, prop) BW(y, D, prop, nu=81),
    ELW
)

#' Imputation model: Logistic
Non_DR_Estimators.logistic = c(
    IPW,
    SIPW_POP,
    SIPW_NR,
    Strat_Pi_matrix,
    Strat_Pi_y,
    IPW.zzz,
    IPW.chim,
    IPW.trim,
    OW,
    MW,
    EW,
    BW_11 = function(y, D, prop) BW(y, D, prop, nu=11),
    BW_81 = function(y, D, prop) BW(y, D, prop, nu=81),
    ELW    
)
```

```{r}
# AIPW = function(y, D, prop, y_imputed)
# AIPW_Stablized = function(y, D, prop, y_imputed)
# WLS = function(y, D, x, prop, impute_model_formula) <- linear imputation
# Pi_Cov_1 = function(y, D, x, prop, impute_model_formula) <- linear imputation
# Pi_Cov_2 = function(y, D, x, prop, impute_model_formula) <- linear imputation
# TMLE = function(y, D, x, prop_model_formula, impute_model_formula)
# DR_BR = function(y, D, x, type) <- linear/logistic imputation
# ELW_DR = function(y, D, prop, y_imputed)
# DR_Bounded_IPW = function(y, D, x, prop_model_formula, impute_model_formula)
# DR_Bounded_Regression = function(y, D, x, prop, impute_model_formula) <- linear imputation
# CBPS = function(y, D, x, prop_model_formula)
# CTD = function(y, D, x, prop_model_formula, impute_model_formula) <- linear imputation
# ZZ = function(y, D, prop, y_imputed, hn_power) -> hn_power = -1/3, -1/4 or -1/5

#' Imputation model: Linear
DR_Estimators.linear = c(
    AIPW,
    AIPW_Stablized,
    WLS,
    Pi_Cov_1,
    Pi_Cov_2,
    TMLE,
    DR_BR = function(y, D, x) DR_BR(y, D, x, type="linear"),
    ELW_DR,
    DR_Bounded_IPW,
    DR_Bounded_Regression,
    CBPS,
    CTD,
    ZZ_13 = function(y, D, prop, y_imputed) ZZ(y, D, prop, y_imputed, hn_power=-1/3),
    ZZ_15 = function(y, D, prop, y_imputed) ZZ(y, D, prop, y_imputed, hn_power=-1/5)
)


#' Imputation model: Logistic
DR_Estimators.logistic = c(
    AIPW,
    AIPW_Stablized,
    TMLE,
    DR_BR = function(y, D, x) DR_BR(y, D, x, type="logistic"),
    ELW_DR,
    DR_Bounded_IPW,
    CBPS,
    ZZ
)
```

```{r}

```

```{r}

```

TODO: Make a nice plot comparing all parameters!


# Questions

1. Is it possible to include real data analysis such as Lalonde data for doubly robust case?
2. Do I need to use Bootstrap for standard deviation, rather than the direct procedure here?
3. Two methods I don't know how to implement yet:
    + Rotnitzky, A., et al. “Improved Double-Robust Estimation in Missing Data and Causal Inference Models.” Biometrika, vol. 99, no. 2, June 2012, pp. 439–56. DOI.org (Crossref), https://doi.org/10.1093/biomet/ass013.
    + “Multiply Robust Imputation Procedures for the Treatment of Item Nonresponse in Surveys.” Biometrika, 2017. DOI.org (Crossref), https://doi.org/10.1093/biomet/asx007.
