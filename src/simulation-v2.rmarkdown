---
title: 'Double Robust version of ELW: Simulation-2'
author: 'Mingcheng Hu'
---


# Set Up Environment


```{r}
install.packages("pbmcapply", repos = "https://cran.r-project.org")
```

```{r include = FALSE}
library(MASS)
library(knitr)
library(knitrProgressBar)
library(ggplot2)
library(dplyr)
library(parallel)
library(pbmcapply)  # wrap mclapply with progress bar

set.seed(1234)
options(digits = 6)

num_cores = detectCores() - 1
```

```{r label = "Constants"}
nrep_large = 2000
nrep_small = 500

N_large = 100000
N_normal = 20000
N_small = 5000

n_rep = nrep_small  # modify for different number of replications
N = N_normal  # modify for different size of data
```




# Implementing Estimators


```{r}
expit = function(x) 1 / (1 + exp(-x))
logit = function(p) log(p / (1 - p))
```


## Non-DR Estimators


```{r label = "Non-DR Estimators Definitions"}
#' Missing model: Any
#' Imputation model: Any
IPW = function(y, D, prop) {
    N = length(y)
    return(sum(D / prop * y) / N)
}
```

```{r}
# Ref Kang, Joseph D. Y., and Joseph L. Schafer. “Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data.” Statistical Science, vol. 22, no. 4, Nov. 2007. DOI.org (Crossref), https://doi.org/10.1214/07-STS227.


# Ref (3), Table 1
# Reweighting to resemble the full population
#' Missing model: Any
#' Imputation model: Any
SIPW_POP = function(y, D, prop) { # prop is estimated using missing model
    return(sum(D / prop * y) / sum(D / prop)) # modified IPW
}

# Ref (4), Table 1
# Reweighting to resemble the nonrespondents, return the estimate of full population
#' Missing model: Any
#' Imputation model: Any
SIPW_NR = function(y, D, prop) {
    r1 = mean(D) # proportion of respondents
    r0 = mean(1 - D)
    # * g = 1-e(x) in A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights
    mu_NR = sum(D / prop * (1 - prop) * y) / sum(D / prop * (1 - prop))  # * y0 in A Framework for Causal Inference...
    return(r1 * mean(y[D == 1]) + r0 * mu_NR)
}

# Ref (6), Table 2
# We use the matrixed version below for faster computation
#' Missing model: Any
#' Imputation model: Any
Strat_Pi = function(y, D, prop, S = 5) {
    N = length(y)
    si = cut(prop, breaks = S, labels = 1:S) # each individual's stratum
    ci = matrix(NA, nrow = N, ncol = S) # indicator matrix
    for (s in 1:S) {
        for (i in 1:N) {
            ci[i, s] = (si[i] == s)
        }
    }
    strata = rep(NA, S)
    for (s in 1:S) {
        strata[s] = (sum(ci[, s]) / N) * (sum(ci[, s] * D * y) / sum(sum(ci[, s] * D)))
    }
    return(sum(strata))
}

Strat_Pi_matrix = function(y, D, prop, S = 5) {
    N = length(y)
    si = cut(prop, breaks = S, labels = 1:S)
    ci = sapply(1:S, function(s) si == s)
    strata = colSums(ci) / N * colSums(ci * D * y) / colSums(ci * D)
    return(sum(strata))
}

# Ref (7), Table 3
# * See comment for demystifing paper why it works well
#' Missing model: Any
#' Imputation model: Linear
OLS = function(y, D, x, impute_model_formula) {
    data = data.frame(y = y, D = D, x)
    impute_model = lm(impute_model_formula, data = subset(data, D == 1))
    return(mean(predict(impute_model, newdata = data)))
}

# Ref Table 4
#' Missing model: Any
#' Imputation model: Any
Strat_Pi_y = function(y, D, prop, y_imputed, S = 5) {
    si_pi = cut(prop, breaks = S, labels = 1:S)
    si_y = cut(y_imputed, breaks = S, labels = 1:S)

    table_si = table(si_pi, si_y)
    cell_means = matrix(NA, nrow = S, ncol = S)
    cell_n = matrix(NA, nrow = S, ncol = S)

    for (i in 1:S) {
        for (j in 1:S) {
            # All individuals in the cell
            cell_indices = which(si_pi == i & si_y == j)

            cell_n[i, j] = length(cell_indices)
            if (length(cell_indices) > 0) {
                t1_indices_cell = cell_indices[D[cell_indices] == 1]

                if (length(t1_indices_cell) > 0) {
                    cell_means[i, j] = mean(y[t1_indices_cell])
                }
            }
        }
    }

    # Handle offending cells (cells that only has non-respondents)
    for (i in 1:S) {
        for (j in 1:S) {
            # If the cell mean is NA (offending cell)
            if (is.na(cell_means[i, j]) && cell_n[i, j] > 0) {
                # Here we use imputation model assuming only row and column effects (no interaction)
                row_effect = mean(cell_means[i, ], na.rm = TRUE) # Row-wise mean
                col_effect = mean(cell_means[, j], na.rm = TRUE) # Column-wise mean
                overall_mean = mean(cell_means, na.rm = TRUE) # Overall mean
                cell_means[i, j] = row_effect + col_effect - overall_mean
            }
        }
    }

    cell_means_vector = as.vector(cell_means)
    cell_n_vector = as.vector(cell_n)
    return(weighted.mean(cell_means_vector, cell_n_vector, na.rm = TRUE))
}
```

```{r}
# Ref Zong, Xianpeng, et al. Improved Horvitz-Thompson Estimator in Survey Sampling. arXiv:1804.04255, arXiv, 11 Apr. 2018. arXiv.org, http://arxiv.org/abs/1804.04255.
# Ref Section 3
# * Truncated estimator (down-weighting)
#' Missing model: Any
#' Imputation model: Any
IPW.zzz = function(y, D, prop) {
    N = length(y)
    index = 1:N
    tmp = sort(prop)
    i = max(which(tmp < 1 / (index + 1))) # * hard-threshold based on rank
    crit = tmp[i]

    prop.star = prop
    prop.star[which(prop <= crit)] = crit
    return(sum(D * y / prop.star) / N)
}

# Ref Crump R. K., Hotz V. J., Imbens G. W., & Mitnik O. A. (2009). Dealing with limited overlap in estimation of average treatment effects. http://doi.org/10.1093/biomet/asn055
# * Trimmied estimator (dicarding)
#' Missing model: Any
#' Imputation model: Any
IPW.chim = function(y, D, prop) {
    tempfun = function(a) {
        u = prop * (1 - prop)
        v = a * (1 - a)
        z = (u >= v)
        a + (2 * v * sum(z / u) > sum(z))
    }
    a = optimize(tempfun, lower = 1e-5, upper = 0.5, maximum = FALSE)$minimum

    N = length(y)
    y = rep(y, D)
    pw = rep(prop, D)

    ind = (pw > a & pw < 1 - a)
    y.star = y[ind]
    pw.star = pw[ind]
    return(sum(y.star / pw.star) / sum(1 / pw.star))
}

# Ref Ma, Xinwei, and Jingshen Wang. Robust Inference Using Inverse Probability Weighting. arXiv:1810.11397, arXiv, 24 May 2019. arXiv.org, http://arxiv.org/abs/1810.11397.
# Ref Section 3.2
# * Trimmed estimator
#' Missing model: Any
#' Imputation model: Any
IPW.trim = function(y, D, prop, s) {
    N = length(D)
    n = sum(D)

    # Ref Theorem2
    fun1 = function(t, s) {
        2 * n * t^s * mean(prop <= t) - 1
    }
    # define Trimming threshold
    bn1 = uniroot(fun1, c(0, 1), s = 1)$root # moderate trimming
    bn2 = uniroot(fun1, c(0, 1), s = 2)$root # heavy trimming

    # define Bandwidth sequence for local polynomial regression
    # Ref Supplementary Material II.1
    fun2 = function(t) {
        n * t^5 * mean(prop <= t) - 1
    }
    hn = uniroot(fun2, c(0, 1))$root

    y1 = rep(y, D) # observed Y
    pw = rep(prop, D)
    xx = cbind(rep(1, n), pw)
    ww = (pw < hn) # We use subsample in region (0,hn) for local polynomial regression
    xx1 = cbind(ww, ww * pw)
    xx.all = cbind(rep(1, N), prop)

    # Ref Algorithm1, Step1
    beta = ginv(t(xx1) %*% xx) %*% t(xx1) %*% y1

    # define Bias correction terms
    # Ref Algorithm1, Step2
    Bnb1 = -mean((xx.all %*% beta) * (prop < bn1))
    ipw.trim1 = mean(y1 * (pw >= bn1) / pw) * n / N
    ipw.bc1 = ipw.trim1 - Bnb1

    Bnb2 = -mean((xx.all %*% beta) * (prop < bn2))
    ipw.trim2 = mean(y1 * (pw >= bn2) / pw) * n / N
    ipw.bc2 = ipw.trim2 - Bnb2

    if (s == 1) {
        return(ipw.bc1)
    } else if (s == 2) {
        return(ipw.bc2)
    } else {
        stop("Invalid s. s must be 1 or 2")
    }
}
```

```{r}
# Ref Matsouaka, Roland A., and Yunji Zhou. A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights: The Role of Overlap Weights. arXiv:2011.01388, arXiv, 24 Oct. 2022. arXiv.org, http://arxiv.org/abs/2011.01388.
# Ref 3.2.2


# * We use stablized version, similar to the one in demystifing paper
#' Missing model: Any
#' Imputation model: Any
OW = function(y, D, prop) {
    g = prop * (1 - prop)
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}

#' Missing model: Any
#' Imputation model: Any
MW = function(y, D, prop) {
    g = min(prop, 1 - prop)
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}

#' Missing model: Any
#' Imputation model: Any
EW = function(y, D, prop) {
    g = -(prop * log(prop) + (1 - prop) * log(1 - prop))
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}

#' Missing model: Any
#' Imputation model: Any
# * We follow the simulation in 4.5.2 and set nu = 11 (smaller SE) or 81 (smaller bias)
BW = function(y, D, prop, nu) {
    if (nu < 2) {
        stop("nu must be greater than or equal to 2")
    }

    g = (prop * (1 - prop)) ^ nu
    y_overlap = sum(D / prop * g * y)
    y_overlap = y_overlap / sum(D / prop * g)
    return(mean(y_overlap))
}
```

```{r}
# Ref Liu, Yukun. “Biased-Sample Empirical Likelihood Weighting for Missing Data Problems: An Alternative to Inverse Probability Weighting.” Statistical Methodology, vol. 85, no. 1, 2023.
#' Missing model: Any
#' Imputation model: Any
ELW = function(y, D, prop, tol = sqrt(.Machine$double.eps)) { # tol is the tolerance level
    N = length(y)
    n = sum(D)
    y1 = y[D == 1]
    prop1 = prop[D == 1] # propensity score of respondents
    low = min(prop1)
    xi = n / N + (1 - n / N) * prop1
    up = min(xi) - tol

    fun_alpha = function(alpha) {
        sum((prop1 - alpha) / (xi - alpha))
    }
    alpha = uniroot(fun_alpha, interval = c(low, up), tol = tol)$root
    lambda = (N / n - 1) / (1 - alpha)
    tmp = 1 + lambda * (prop1 - alpha)
    prob.elw = 1 / (n * tmp)

    return(sum(y1 * prob.elw))
}
```


## DR Estimators


```{r label = "DR Estimators Definitions"}
# * This is also BC (bias corrected)-OLS in demystifing paper, Table 5
# * This is also mu_DR(pi, m_REG) in the comment for demystifing paper
#' Missing model: Any
#' Imputation model: Any
AIPW = function(y, D, prop, y_imputed) {
    N = length(y)
    return(mean(y_imputed) + sum(D / prop * (y - y_imputed)) / N)
}

# B-DR in comment for demystifing paper
#' Missing model: Any
#' Imputation model: Any
AIPW_Stablized = function(y, D, prop, y_imputed) {
    N = length(y)
    return(mean(y_imputed) + sum(D / prop * (y - y_imputed)) / sum(D / prop))
}
```

```{r}
# Ref Kang, Joseph D. Y., and Joseph L. Schafer. “Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data.” Statistical Science, vol. 22, no. 4, Nov. 2007. DOI.org (Crossref), https://doi.org/10.1214/07-STS227.

# Ref (10), Table 6
#' Missing model: Any
#' Imputation model: Linear
WLS = function(y, D, x, prop, impute_model_formula) {
    data = data.frame(y = y, D = D, x, prop = prop)
    impute_model = lm(impute_model_formula, data = subset(data, D == 1), weights = 1 / prop)
    return(mean(predict(impute_model, newdata = data)))
}

# Ref Table 7
# * Coarsening eta into five categories and creating dummy indicators
#' Missing model: Any
#' Imputation model: Linear
Pi_Cov_1 = function(y, D, x, prop, impute_model_formula) {
    eta = expit(prop)
    eta_cat = cut(eta, breaks = 5, labels = 1:5)
    eta_dummy = model.matrix(~ eta_cat)[, -1]  # one-hot encoding, four dummy variables
    colnames(eta_dummy) = paste("eta", 2:5, sep = "_")
    data = data.frame(y = y, D = D, x, prop = prop, eta_dummy)
    # * Don't directly use paste for extension, use update instead
    impute_model_formula_extended = update(impute_model_formula, ~ . + eta_2 + eta_3 + eta_4 + eta_5)
    impute_model = lm(impute_model_formula_extended, data = subset(data, D == 1))
    return(mean(predict(impute_model, newdata = data)))
}

# Ref Table 8
# * This is also mu_DR(pi, m_EXTREG) in the comment for demystifing paper
#' Missing model: Any
#' Imputation model: Linear
Pi_Cov_2 = function(y, D, x, prop, impute_model_formula) {
    prop_inv = 1 / prop
    impute_model_formula_extended = update(impute_model_formula, ~ . + I(prop_inv))
    data = data.frame(y = y, D = D, x, prop = prop, prop_inv)
    impute_model = lm(impute_model_formula_extended, data = subset(data, D == 1))
    return(mean(predict(impute_model, newdata = data)))
}
```

```{r}
library(tmle) # Tested on version 1.3.0-1, not working on latest version
# Ref Gruber, Susan, and Mark J. Van Der Laan. “Tmle : An R Package for Targeted Maximum Likelihood Estimation.” Journal of Statistical Software, vol. 51, no. 13, 2012. DOI.org (Crossref), https://doi.org/10.18637/jss.v051.i13.
# Ref Page 17 (Mis-specified imputation model)
#' Missing model: Any
#' Imputation model: Any
TMLE = function(y, D, x, prop_model_formula, impute_model_formula) {
    N = length(y)
    y[D == 0] = NA
    # Response variables must be Y and Delta
    prop_model_formula_renamed = as.formula(
        gsub("D", "Delta", deparse(prop_model_formula))
    )
    impute_model_formula_renamed = as.formula(
        gsub("y", "Y", deparse(impute_model_formula))
    )
    result.EY1 = tmle(y,
        A = rep(1, N), x,
        Qform = impute_model_formula_renamed, 
        g.Deltaform = prop_model_formula_renamed, 
        Delta = D
    )
    return(result.EY1$estimates$EY1$psi)
}
```

```{r}
n <- 250
W <- matrix(rnorm(n * 3), ncol = 3)
colnames(W) <- paste("W",1:3, sep = "")
Delta <- rbinom(n, 1, plogis(0.8 + 0.3*W[,1]))
Y <- 2 * W[,1] + 4 * W[,2] + 3 * W[,3]+ rnorm(n)
Y[Delta == 0] <- NA
result.EY1 <- tmle(Y,A = rep(1, n), W, Qform = Y ~ W3, g.Deltaform = Delta ~ W1, Delta = Delta)
result.EY1
```

```{r}
# Ref Vermeulen, Karel, and Stijn Vansteelandt. “Bias-Reduced Doubly Robust Estimation.” Journal of the American Statistical Association, vol. 110, no. 511, July 2015, pp. 1024–36. DOI.org (Crossref), https://doi.org/10.1080/01621459.2014.958155.
# Ref Section 3.3, Appendix J
# * Missing model: Logistic
# * Conditional mean model: Linear/Logistic
DR_BR = function(y, D, x, prop_model_formula, impute_model_formula, type) {
    N = length(y)

    # * Make sure two models have the same set of covariates
    prop_model_vars = all.vars(prop_model_formula)[-1]
    impute_model_vars = all.vars(impute_model_formula)[-1]
    all_vars <- unique(c(prop_model_vars, impute_model_vars))

    prop_model_formula_extended <- update(prop_model_formula, paste(". ~", paste(all_vars, collapse = " + ")))
    impute_model_formula_extended <- update(impute_model_formula, paste(". ~", paste(all_vars, collapse = " + ")))

    x_used = x[, all_vars, drop=FALSE]

    xx = cbind(rep(1, N), x_used)
    data = data.frame(y = y, D = D, x_used)

    # define Functions to obtain an estimator of gamma
    # Linear conditional mean model
    min.Uint.linear = function(gamma) {
        -mean((-D * exp(-gamma %*% t(xx))) + (-(1 - D) * (gamma %*% t(xx))))
    }

    # Logistic conditional mean model
    min.Uint.logistic = function(gamma) {
        -mean(((-R * exp(-gamma %*% t(xx))) + (-(1 - D) * (gamma %*% t(xx)))) *
            as.vector(expit(init.beta %*% t(xx)) * (1 - expit(init.beta %*% t(xx)))))
    }

    # This is exactly AIPW (RI + correction factor)
    U = function(y, D, X, gamma, beta) {
        (beta %*% t(X) + D / expit(gamma %*% t(X)) * (y - beta %*% t(X)))
    }

    # init.gamma = coef(glm(D ~ x, family = "binomial"))
    init.gamma = coef(glm(prop_model_formula, family = "binomial", data = data))
    if (type == "linear") {
        sol = nlm(min.Uint.linear, init.gamma) # nonlinear minimization
    } else if (type == "logistic") {
        sol = nlm(min.Uint.logistic, init.gamma)
    } else {
        stop("Invalid type")
    }
    # define Parameters of missing model
    gamma.BR = sol$estimate

    weight = as.vector(1 / exp(gamma.BR %*% t(xx)))
    data = data.frame(y = y, D = D, x, weight)  # make sure variable lengths are the same
    # define Parameter of imputation model
    beta.BR = coef(lm(impute_model_formula_extended, data = subset(data, D == 1), weights = weight)) # only consider respondents

    # define Estimate of mean outcome
    mn.y = mean(U(y, D, xx, gamma.BR, beta.BR))
    return(mn.y)
}
```

```{r}
m.biasreducedDR.identity<-function(R,Y,cov){ 
    n<-length(R) 
    int.cov<-cbind(rep(1,n),cov) 
    expit<-function(x) exp(x)/(1+exp(x)) 
    U <- function(R,Y,X,gamma,beta)
        { 
            (R/expit(gamma%*%t(X))*(Y-beta%*%t(X))+beta%*%t(X)) 
        }  
    min.Uint<-function(gamma){ 
        -mean((-R*exp(-gamma%*%t(int.cov)))+(-(1-R)*(gamma%*%t(int.cov)))) 
    }  
    init.gamma <-coef(glm(R ~ cov,family="binomial")) 
    sol<-nlm(min.Uint,init.gamma) 
    gamma.BR<-sol$estimate 
    weight<-as.vector(1/exp(gamma.BR%*%t(int.cov))) 
    beta.BR<-coef(lm(Y ~ -1+int.cov,subset=(R==1),weights=weight))  
    mn.Y<-mean(U(R,Y,int.cov,gamma.BR,beta.BR)) 
    return(list(mn.Y=mn.Y)) 
}
```

```{r}
# * Proposed estimator
ELW_DR = function(y, D, prop, y_imputed, eps = sqrt(.Machine$double.eps)) { # eps is the tolerance level
    N = length(y)
    n = sum(D)

    # Only consider respondents
    y1 = y[D == 1]
    # y1_imputed = y_imputed[D == 1]
    prop1 = prop[D == 1]

    low = min(prop1)
    xi = n / N + (1 - n / N) * prop1
    up = min(xi) - eps

    fun_alpha = function(alpha) {
        sum((prop1 - alpha) / (xi - alpha))
    }
    alpha = uniroot(fun_alpha, interval = c(low, up), tol = eps)$root
    lambda = (N / n - 1) / (1 - alpha)
    tmp = 1 + lambda * (prop1 - alpha)
    prob.elw1 = 1 / (n * tmp)
    prob.elw = rep(0, N)
    prob.elw[D == 1] = prob.elw1

    # * Note here we can use prob.elw * y as prob.elw is only nonzero for respondents
    return(sum(prob.elw * y) + mean(y_imputed) - sum(prob.elw * y_imputed))
}
```

```{r}
# Ref Robins, James, et al. “Comment: Performance of Double-Robust Estimators When ‘Inverse Probability’ Weights Are Highly Variable.” Statistical Science, vol. 22, no. 4, Nov. 2007. DOI.org (Crossref), https://doi.org/10.1214/07-STS227D.

# AIPW estimator can be expressed as mu_DR(pi, m_REG)

# Ref IPW DR Estimator: mu_DR(pi_EXT, m_REG)
#' Missing model: Any
#' Imputation model: Linear
DR_Bounded_IPW = function(y, D, x, prop_model_formula, impute_model_formula) {
    data = data.frame(y = y, D = D, x)
    mu_OLS = OLS(y, D, x, impute_model_formula)
    y_imputed = predict(lm(impute_model_formula, data = subset(data, D == 1)), newdata = data)
    h = y_imputed - mu_OLS  # default choice in paper for extending missing model
    # prop_model_formula_extended = paste(prop_model_formula, "+ h")
    prop_model_formula_extended = update(prop_model_formula, . ~ . + h)
    # update data
    data$h = h
    prop = glm(prop_model_formula_extended, family = binomial, data=data)$fitted.values
    return(AIPW_Stablized(y, D, prop, y_imputed))
}

# Ref Regression DR Estimator1: mu_DR(pi, m_EXTREG)
# Already implemented above as Pi_Cov_2

# Ref Regression DR Estimator2: mu_DR(pi, m_WLS)
# Already implemented above as WLS

# Ref Regression DR Estimator3: mu_DR(pi, m_DR_IPW_NR)
# * Adding the covariate pi instead of adding 1/pi doesn't induce model extrapolation problems
#' Missing model: Any
#' Imputation model: Linear
DR_Bounded_Regression = function(y, D, x, prop, impute_model_formula) {
    impute_model_formula_extended = update(impute_model_formula, . ~ . + prop)
    data = data.frame(y = y, D = D, x, prop = prop)
    impute_model = lm(impute_model_formula_extended, data = subset(data, D == 1))
    return(mean(predict(impute_model, newdata = data)))
}
```

```{r}
library(nleqslv)
# Ref Imai, Kosuke, and Marc Ratkovic. “Covariate Balancing Propensity Score.” Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 76, no. 1, Jan. 2014, pp. 243–63. DOI.org (Crossref), https://doi.org/10.1111/rssb.12027.

CBPS = function(y, D, x, prop_model_formula) {
    # Follow the code in https://github.com/kosukeimai/CBPS/blob/master/R/CBPSMain.R
    data = data.frame(y = y, D = D, x)
    CBPS.fit = CBPS::CBPS(prop_model_formula, data = data, ATT = 0)
    return(mean(D*y/CBPS.fit$fitted.values))
}

# Ref Cao, Weihua, et al. “Improving Efficiency and Robustness of the Doubly Robust Estimator for a Population Mean with Incomplete Data.” Biometrika, vol. 96, no. 3, Sept. 2009, pp. 723–34. DOI.org (Crossref), https://doi.org/10.1093/biomet/asp033.
# Ref Equation 16
#' Missing model: Logistic
#' Imputation model: Linear
CTD = function(y, D, x, prop_model_formula, impute_model_formula) {
    data = data.frame(y = y, D = D, x)  # Don't put x=x to keep separate columns
    prop = glm(prop_model_formula, family = binomial, data=data)$fitted.values
    impute_model = lm(impute_model_formula, data = subset(data, D == 1))
    y_imputed = predict(impute_model, newdata = data)

    # p+q dimensions in total
    x_impute = model.matrix(impute_model_formula, data = data.frame(y, x))
    p = ncol(x_impute)
    m_beta = x_impute  # derivative of imputation model w.r.t beta
    x_prop = model.matrix(prop_model_formula, data = data.frame(D, x))
    q = ncol(x_prop)
    pi_theta = prop * (1 - prop) * x_prop    # derivative of missing model w.r.t theta

    term1 = D / prop   # N*1
    term2 = (1 - prop) / prop  # N*1
    term3.1 = m_beta  # N*p
    term3.2 = pi_theta / (1 - prop)  # N*q

    estimating_equations <- function(params) {
        beta <- params[1:p]
        c <- params[(p + 1):(p + q)]

        y_imputed_beta = m_beta %*% beta
        term4 = y - y_imputed_beta - (pi_theta %*% c) / (1 - prop)  # N*1

        eq_beta <- colSums(sweep(term3.1, 1, term1 * term2 * term4, `*`))
        eq_c <- colSums(sweep(term3.2, 1, term1 * term2 * term4, `*`))
        # eq_beta <- colSums(term1 * term2 * term3.1 * term4)
        # eq_c <- colSums(term1 * term2 * term3.2 * term4)
        equations = c(eq_beta, eq_c)

        return(equations)
    }

    beta_init = coef(lm(impute_model_formula, data = subset(data, D == 1)))
    init_params <- c(beta_init, rep(0, q))
    solution <- nleqslv(init_params, estimating_equations)

    beta_final = solution$x[1:p]
    c = solution$x[(p + 1):(p + q)]

    y_imputed_final = m_beta %*% beta_final

    # print(beta_init)
    # print(beta_final)
    return(AIPW(y, D, prop, y_imputed_final))
}

# Ref Zhang, Min, and Baqun Zhang. “A Stable and More Efficient Doubly Robust Estimator.” Statistica Sinica, 2022. DOI.org (Crossref), https://doi.org/10.5705/ss.202019.0265.
# * We use the setting in Section 4, where there are three options of hn
# * For simplicity, we use Gassian as the Nadaraya-Watson kernel
#' Missing model: Any
#' Imputation model: Any
ZZ = function(y, D, prop, y_imputed, hn_power, progress_bar = FALSE) { # h_n is the bandwidth
    if (!hn_power %in% c(-1/3, -1/4, -1/5)) {
        stop("hn_power must be one of -1/3, -1/4, -1/5")
    }

    # * We use Gaussian kernel for simplicity
    kernel_function <- function(u) {
        exp(-0.5 * u^2) / sqrt(2 * pi)  
    }

    N = length(y)
    h_n = N^hn_power

    numerator_sum <- 0
    denominator_sum <- 0
  
    if(progress_bar) {
        pb = progress_estimated(N)
    }
    for (i in 1:N) {
        if (progress_bar) {
            update_progress(pb)
        }
        numerator_i <- 0
        denominator_i <- 0
        for (j in 1:N) {
            kernel_weight <- kernel_function((prop[j] - prop[i]) / h_n)
            numerator_i <- numerator_i + D[j] * (y[j] - y_imputed[j]) * kernel_weight
            denominator_i <- denominator_i + D[j] * kernel_weight
        }
        numerator_sum <- numerator_sum + (numerator_i / denominator_i) + y_imputed[i]
    }

    # population mean
    mu <- numerator_sum / N
    return(mu)
}

# * Even though it is optimized, the O(N^2) makes it very unappealing for large N
ZZ_optimized <- function(y, D, prop, y_imputed, hn_power) {
    if (!hn_power %in% c(-1/3, -1/4, -1/5)) {
        stop("hn_power must be one of -1/3, -1/4, -1/5")
    }

    kernel_function <- function(u) {
        exp(-0.5 * u^2) / sqrt(2 * pi)
    }

    N <- length(y)
    h_n <- N^hn_power

    u_matrix <- outer(prop, prop, FUN = function(pi, pj) (pj - pi) / h_n)
    kernel_weights <- kernel_function(u_matrix)
    D_matrix <- matrix(D, nrow = N, ncol = N, byrow = TRUE)
    numerator_matrix <- D_matrix * (y - y_imputed) * kernel_weights
    denominator_matrix <- D_matrix * kernel_weights
    numerator <- rowSums(numerator_matrix)
    denominator <- rowSums(denominator_matrix)
    result <- numerator / denominator + y_imputed
    mu <- mean(result)
    return(mu)
}
```



# Various Data Generation Mechanisms with Extreme with Extreme Weights

For the incorrect models, we follow the convention in *A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights* and omit the first covariate.

In order to make use of the estimators defined above, we need to supply `prop`, `y_imputed`, `prop_model_formula` and `impute_model_formula` for each data generation mechanism. We will provide a list through `fitted_model`.



```{r}
Fitted.model0 = setRefClass("Fitted.model0",
    fields = list(
        y = "numeric",  # length is N
        D = "numeric",
        data = "data.frame"
    ),
    methods = list(
        get_x = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_prop_model_formula.correct = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_prop_model_formula.incorrect = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_impute_model_formula.correct = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_impute_model_formula.incorrect = function() {
            stop("This method needs to be overridden in the subclass")
        },
        get_prop.correct = function() {
            prop = glm(get_prop_model_formula.correct(), family = binomial, data = data)$fitted.values
            return(prop)
        },
        get_prop.incorrect = function() {
            prop = glm(get_prop_model_formula.incorrect(), family = binomial, data = data)$fitted.values
            return(prop)
        },
        get_y_imputed.correct = function() {
            impute_model = lm(get_impute_model_formula.correct(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function() {
            impute_model = lm(get_impute_model_formula.incorrect(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```


## Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data & Comment: Performance of Double-Robust Estimators When "Inverse Probability" Weights Are Highly Variable

The simulation 1 is proposed in demystifing paper. This is a standard scenario and has been widely used in a variety of related papers.


```{r}
#' Model mis-speification: Wrong covariates
data.gen1 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    y = 210 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4 + epsilon
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)

    # wrong covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    data = list(
        y = y, D = D,
        z1 = z1, z2 = z2, z3 = z3, z4 = z4,
        x1 = x1, x2 = x2, x3 = x3, x4 = x4
    )
    return(list(data = data, prop = prop))
}

# Example:  `fitted.model1 = do.call(Fitted.model1$new, data)`
Fitted.model1 = setRefClass("Fitted.model1",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        }
    )
)
```



The simulation 2 is a modification of the original one. We change the setting from $D=1$ being the respondents to $D=0$ being the respondents. This is proposed in the comment paper, designed so that OLS estimator no longer performs better than DR estimators.



```{r}
#' Model mis-speification: Wrong covariates
data.gen2 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    y = 210 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4 + epsilon
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)
    # * We analyze the data in which Y is observed only when D = 0
    # This should lead to weights of opposite signs
    D = 1 - D

    # covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    data = list(
        y = y, D = D,
        z1 = z1, z2 = z2, z3 = z3, z4 = z4,
        x1 = x1, x2 = x2, x3 = x3, x4 = x4
    )
    return(
        list(data = data, prop = 1 - prop)
    )
}

# Same as Fitted.model1
Fitted.model2 = setRefClass("Fitted.model2",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        }
    )
)
```




## A Framework for Causal Inference in the Presence of Extreme Inverse Probability Weights

**We shall not address the original treatment effect estimation problem. Instead, we take the data as missing data and wish to estimate the average earnings of the treated.**

### Illustrative Example

This is a modified example of the original one. We set $Y=Y_1$ so that the response no longer depends on $D$.


```{r}
alpha_A.gen3 = c(-2.8, 0.2, 0.8) # p = 20% (extreme weight)
alpha_B.gen3 = c(-1.6, 0.45, 0.6) # p = 50%
alpha_C.gen3 = c(0.2, 0.8, 0.2) # p = 80%

#' Model mis-speification: Ignore covariates (Set to first one)
data.gen3 = function(N, alpha = alpha_A.gen3) {
    x1 = rnorm(N, 2, 2)
    x2 = rnorm(N, 1, 1)

    prop = 1 / (1 + exp(-(alpha[1] + alpha[2] * x1 + alpha[3] * x2)))
    D = rbinom(N, 1, prop)

    epsilon1 = rnorm(N, 0, 2)
    # epsilon2 = rnorm(N, 0, 2)
    # y = ifelse(D == 1, 2 + x1 + x2 + 2*x1^2 + 0.5*x2^2 + epsilon1, x1 + x2 + epsilon2)
    y = 2 + x1 + x2 + 2 * x1^2 + 0.5 * x2^2 + epsilon1

    data = list(
        y = y, D = D,
        x1 = x1, x2 = x2
    )
    return(list(data = data, prop = prop))
}

Fitted.model3 = setRefClass("Fitted.model3",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        data = "data.frame"       
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x2"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + I(x1^2) + I(x2^2)"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + I(x2^2)"))
        }
    )
)
```


### First Simulation

In this case we choose the homogeneous treatment effect $\Delta=3$. Note that as we modify the setting, this will be incorporated as the intercept of the conditional mean model.


```{r}
alpha_A.gen4 = c(-0.5, 0.3, 0.4, 0.4, 0.4) # good overlap
alpha_B.gen4 = c(-1, 0.6, 0.8, 0.8, 0.8) # moderate overlap
alpha_C.gen4 = c(-1.5, 0.9, 1.2, 1, 2, 1.2) # poor overlap

#' Model mis-speification: Ignore covariates (Set to first one)
data.gen4 = function(N, alpha = alpha_C.gen4) {
    x4 = rbinom(N, 1, 0.5)
    x3 = rbinom(N, 1, 0.4 + 0.2 * x4)
    mu_matrix = cbind(x4 - x3 + 0.5 * x3 * x4, -x4 + x3 + x3 * x4)
    Sigma_elements = cbind(2 - x3, 0.25 * (1 + x3))
    x1x2 = t(sapply(1:N, function(i) {
        Sigma_matrix = matrix(c(
            Sigma_elements[i, 1], Sigma_elements[i, 2],
            Sigma_elements[i, 2], Sigma_elements[i, 1]
        ), 2, 2)
        mvrnorm(1, mu = mu_matrix[i, ], Sigma = Sigma_matrix)
    }))
    x1 = x1x2[, 1]
    x2 = x1x2[, 2]

    prop = 1 / (1 + exp(-(alpha[1] + alpha[2] * x1 + alpha[3] * x2 + alpha[4] * x3 + alpha[5] * x4)))
    D = rbinom(N, 1, prop)

    epsilon = rnorm(N, 0, 1)
    y = 0.5 + x1 + 0.6 * x2 + 2.2 * x3 + 1.2 * x4 + epsilon

    data = list(
        y = y, D = D,
        x1 = x1, x2 = x2, x3 = x3, x4 = x4
    )
    return(list(data = data, prop = prop))
}

Fitted.model4 = setRefClass("Fitted.model4",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + x3 + x4"))
        }
    )
)
```


### Second Simulation

In this case we choose the heterogeneous treatment effect $\Delta=prop^2+2prop+1$. The simulation focsues on the impact of proportion of treated participants (prevalence of treatment)


```{r}
alpha.gen5 = c(0.15, 0.3, 0.3, -0.2, -0.25, -0.25)
# low prevalence (around 0.1)
alpha_A.gen5 = c(-2.1, alpha.gen5) # good overlap
alpha_B.gen5 = c(-2.2, 2 * alpha_A.gen5) # moderate overlap
alpha_C.gen5 = c(-2.8, 3 * alpha_A.gen5) # poor overlap

#' Model mis-speification: Ignore covariates (Set to first one)
data.gen5 = function(N, alpha = alpha_C.gen5) {
    mu_matrix = rep(0, 6)
    # unit marginal variance, pairwise covariance of 0.5
    Sigma_matrix = matrix(0.5, nrow = 6, ncol = 6)
    diag(Sigma_matrix) = 1
    x1x2x3x4x5x6 = mvrnorm(N, mu = mu_matrix, Sigma = Sigma_matrix)
    x1 = x1x2x3x4x5x6[, 1]
    x2 = x1x2x3x4x5x6[, 2]
    x3 = x1x2x3x4x5x6[, 3]
    x4 = x1x2x3x4x5x6[, 4]
    x5 = x1x2x3x4x5x6[, 5]
    x6 = x1x2x3x4x5x6[, 6]
    # dichotomize
    x4 = ifelse(x4 > 0, 1, 0)
    x5 = ifelse(x5 > 0, 1, 0)
    x6 = ifelse(x6 > 0, 1, 0)

    prop = 1 / (1 + exp(-(alpha[1] + alpha[2] * x1 + alpha[3] * x2 + alpha[4] * x3 + alpha[5] * x4 + alpha[6] * x5 + alpha[7] * x6)))
    D = rbinom(N, 1, prop)

    epsilon = rnorm(N, 0, 1.5)
    y = -0.5 * x1 - 0.5 * x2 - 1.5 * x3 + 0.8 * x4 + 0.8 * x5 + x6 + epsilon + prop^2 + 2 * prop + 1

    data = list(
        y = y, D = D,
        x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5, x6 = x6
    )
    return(list(data = data, prop = prop))
}

Fitted.model5 = setRefClass("Fitted.model5",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        x5 = "numeric",
        x6 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = x5,
                           x6 = x6)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4 + x5 + x6") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x2 + x3 + x4 + x5 + x6"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4 + x5 + x6 + prop + I(prop^2)"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + x3 + x4 + x5 + x6 + prop"))
        },
        get_y_imputed.correct = function(prop.correct = TRUE) {
            if (prop.correct == TRUE) {
                prop = get_prop.correct()
            } else {
                prop = get_prop.incorrect()
            }
            data$prop <<- prop
            impute_model = lm(get_impute_model_formula.correct(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function(prop.correct = FALSE) {
            if (prop.correct == TRUE) {
                prop = get_prop.correct()
            } else {
                prop = get_prop.incorrect()
            }
            data$prop <<- prop
            impute_model = lm(get_impute_model_formula.incorrect(), data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```


## A Stable and More Efficient Doubly Robust Estimator


```{r}
#' Model mis-speification: Ignore covariates (Set to last one) as in paper
data.gen6 = function(N) {
    x1 = runif(N, 0, 1)
    x2 = rnorm(N)
    x3 = rbinom(N, 1, 0.3)
    x4 = rlnorm(N, 0, 1) # log-normal

    epsilon = rnorm(N)

    prop = expit(-1 - x1 / 2 + x2 - x3 + x4)
    D = rbinom(N, 1, prop)

    y = 2.5 + x1 / 2 + x2 + x3 + x4 + epsilon

    data = list(y = y, D = D, x1 = x1, x2 = x2, x3 = x3, x4 = x4)
    return(list(data = data, prop = prop))
}

Fitted.model6 = setRefClass("Fitted.model6",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x2 + x3 + x4"))
        }
    )
)
```


## Bias-Reduced Doubly Robust Estimation

Scenario 1 is the same as demystifing paper.

### Scenario 2

Incorrect outcome model: $m_0(x)=1+x$
Incorrect propensity model: $\pi_0(x)=expit(\varespilon \cdot x)$


```{r}
#' Model mis-speification: Wrong form of model
data.gen7 = function(N) {
    x = rnorm(N, 0, 1)
    prop = expit(-4 + 1.5 * sqrt(abs(x)) + 0.75 * x + 0.5 * abs(x)^1.5)
    D = rbinom(N, 1, prop)
    y = rnorm(N, x^2, 1)

    data = list(
        y = y, D = D, x = x
    )
    return(list(data = data, prop = prop))
}

Fitted.model7 = setRefClass("Fitted.model7",
    fields = list(
        x = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x = x)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ sqrt(abs(x)) + x + I(abs(x)^1.5)") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x + I(x^2)"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x"))
        }
    )
)
```


## Improved double-robust estimation in missing data and causal inference models

There are four experiments in total:

1. The first one is just the setting in demystifing paper.
2. The second one is just the setting in *Comment: Performance of Double-Robust Estimators When "Inverse Probability" Weights Are Highly Variable* above, where we exchange the missing indicator.
3. The third one modifies the form of Y, changing it from a linear model to a logistic model.
4. The fourth one is the same as the third one, except we exchange the missing indicator as in (2).


```{r}
#' Model mis-speification: Wrong covariates
data.gen8 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    # y = 210 + 27.4*z1 + 13.7*z2 + 13.7*z3 + 13.7*z4 + epsilon
    # * As E[Y]=0.0496, R will report glm.fit: fitted probabilities numerically 0 or 1 occurred
    # * We would like to examine the performance where parameter falls outside of parameter space
    y = rbinom(N, 1, expit(-60 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4)) # change from linar to logistic
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)

    # wrong covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    data = list(
        y = y, D = D,
        z1 = z1, z2 = z2, z3 = z3, z4 = z4,
        x1 = x1, x2 = x2, x3 = x3, x4 = x4
    )
    return(
        list(data = data,prop = prop)
    )
}

Fitted.model8 = setRefClass("Fitted.model8",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_y_imputed.correct = function() {
            impute_model = glm(get_impute_model_formula.correct(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function() {
            impute_model = glm(get_impute_model_formula.incorrect(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```

```{r}
#' Model mis-speification: Wrong covariates
data.gen9 = function(N) {
    z1 = rnorm(N)
    z2 = rnorm(N)
    z3 = rnorm(N)
    z4 = rnorm(N)
    epsilon = rnorm(N)
    # y = 210 + 27.4*z1 + 13.7*z2 + 13.7*z3 + 13.7*z4 + epsilon
    y = rbinom(N, 1, expit(-60 + 27.4 * z1 + 13.7 * z2 + 13.7 * z3 + 13.7 * z4)) # change from linar to logistic
    prop = expit(-z1 + 0.5 * z2 - 0.25 * z3 - 0.1 * z4) # Note that prop can be as small as 0.01
    D = rbinom(N, 1, prop)
    # * We analyze the data in which Y is observed only when D = 0
    D = 1 - D

    # wrong covariates exposed to analyst
    x1 = exp(z1 / 2)
    x2 = z2 / (1 + exp(z1)) + 10
    x3 = (z1 * z3 / 25 + 0.6)^3
    x4 = (z2 + z4 + 20)^2

    data = list(
        y = y, D = D,
        z1 = z1, z2 = z2, z3 = z3, z4 = z4,
        x1 = x1, x2 = x2, x3 = x3, x4 = x4
    )
    return(
        list(data = data, prop = 1 - prop)
    )
}

# Same as Fitted.model8
Fitted.model9 = setRefClass("Fitted.model9",
    fields = list(
        x1 = "numeric",
        x2 = "numeric",
        x3 = "numeric",
        x4 = "numeric",
        z1 = "numeric",
        z2 = "numeric",
        z3 = "numeric",
        z4 = "numeric"
    ),
    contains = "Fitted.model0",
    methods = list(
        initialize = function(...) {
            callSuper(...)
            get_x()
        },
        get_x = function() {
            x = data.frame(x1 = x1, 
                           x2 = x2, 
                           x3 = x3, 
                           x4 = x4,
                           x5 = z1,
                           x6 = z2,
                           x7 = z3,
                           x8 = z4)
            data <<- data.frame(y = y, D = D, x)
            return(x)
        },
        get_prop_model_formula.correct = function() {
            return(as.formula("D ~ x5 + x6 + x7 + x8") )
        },
        get_prop_model_formula.incorrect = function() {
            return(as.formula("D ~ x1 + x2 + x3 + x4"))
        },
        get_impute_model_formula.correct = function() {
            return(as.formula("y ~ x5 + x6 + x7 + x8"))
        },
        get_impute_model_formula.incorrect = function() {
            return(as.formula("y ~ x1 + x2 + x3 + x4"))
        },
        get_y_imputed.correct = function() {
            impute_model = glm(get_impute_model_formula.correct(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        },
        get_y_imputed.incorrect = function() {
            impute_model = glm(get_impute_model_formula.incorrect(), family = binomial, data = subset(data, D == 1))
            return(predict(impute_model, newdata = data))
        }
    )
)
```



# Simulation and Analysis

+ IPW.zzz: IPW Truncated
+ IPW.chim: IPW Trimmed
+ IPW.trim: IPW Trimmed
+ OW, MW, EW, BW: Propensity Scores Methods
+ DR_BR: Estimate Nuisance Parameters
+ DR_Bounded_IPW, DR_Bounded_Regression: DR Estimator with Boundness Property
+ CBPS: Propensity Scores Methods
+ CTD: Estimate Nuisance Parameters
+ ZZ: Avoid Directly Inverse Weights


```{r label = "Utility Functions"}
plot_prop_histogram = function(prop, title) {
    # hist(prop, breaks = 20, main = title, xlab = "Propensity Score")

    fig <- ggplot(data.frame(prop), aes(x = prop)) +
        geom_histogram(bins = 20, fill = "blue", color = "black") +
        labs(title = title, x = "Propensity Score", y = "Frequency")

    ggsave(filename = paste0("img/", title, ".png"), plot = fig, width = 8, height = 6)
}


#' Obtain parameters list
#' @param fitted.model Fitted model
#' @param prop_model Whether the missing model is correct
#' @param impute_model Whether the imputation model is correct
#' @param prop.specify TRUE when dealing with Fitted.model5
#' @param prop.correct Same parameter as in Fitted.model5
obtain_params_list = function(fitted.model, prop_model, impute_model, prop.specify = FALSE, prop.correct = TRUE) {
    if (prop.specify == FALSE) {
        y_imputed.correct = fitted.model$get_y_imputed.correct()
        y_imputed.incorrect = fitted.model$get_y_imputed.incorrect()
    } else {
        # * Specical case for Fitted.model5
        y_imputed.correct = fitted.model$get_y_imputed.correct(prop.correct)
        y_imputed.incorrect = fitted.model$get_y_imputed.incorrect(prop.correct)
    }

    params <- list(
        y = fitted.model$y,
        D = fitted.model$D,
        x = fitted.model$get_x()
    )

    if (prop_model == TRUE) {
        params$prop = fitted.model$get_prop.correct()
        params$prop_model_formula = fitted.model$get_prop_model_formula.correct()
    } else {
        params$prop = fitted.model$get_prop.incorrect()
        params$prop_model_formula = fitted.model$get_prop_model_formula.incorrect()
    }

    if (impute_model == TRUE) {
        params$y_imputed = y_imputed.correct
        params$impute_model_formula = fitted.model$get_impute_model_formula.correct()
    } else {
        params$y_imputed = y_imputed.incorrect
        params$impute_model_formula = fitted.model$get_impute_model_formula.incorrect()
    }

  return(params)
}

#' Obtain results for one simulation, using a list of estimators
#' @param params Parameters list for one simulation
#' @param estimators List of estimators
obtain_results = function(params, estimators) {
    results = lapply(estimators, function(estimator) {
        # avoid "unused argument" error
        arguments = names(formals(estimator))
        filtered_arguments = params[names(params) %in% arguments]
        do.call(estimator, filtered_arguments)
    })
    names(results) = names(estimators)
    return(results)
}
```

```{r label = "Non-DR Estimators Instantiation"}
#' Imputation model: Linear
Non_DR_Estimators.linear = list(
    IPW=IPW,
    SIPW_POP=SIPW_POP,
    SIPW_NR=SIPW_NR,
    Strat_Pi_matrix=Strat_Pi_matrix,
    OLS=OLS,
    Strat_Pi_y=Strat_Pi_y,
    IPW.zzz=IPW.zzz,
    IPW.chim=IPW.chim,
    IPW.trim1 = function(y, D, prop) IPW.trim(y, D, prop, s=1),
    IPW.trim2 = function(y, D, prop) IPW.trim(y, D, prop, s=2),
    OW=OW,
    MW=MW,
    EW=EW,
    BW_11 = function(y, D, prop) BW(y, D, prop, nu=11),
    BW_81 = function(y, D, prop) BW(y, D, prop, nu=81),
    ELW=ELW
)

#' Imputation model: Logistic
Non_DR_Estimators.logistic = list(
    IPW=IPW,
    SIPW_POP=SIPW_POP,
    SIPW_NR=SIPW_NR,
    Strat_Pi_matrix=Strat_Pi_matrix,
    Strat_Pi_y=Strat_Pi_y,
    IPW.zzz=IPW.zzz,
    IPW.chim=IPW.chim,
    IPW.trim1 = function(y, D, prop) IPW.trim(y, D, prop, s=1),
    IPW.trim2 = function(y, D, prop) IPW.trim(y, D, prop, s=2),
    OW=OW,
    MW=MW,
    EW=EW,
    BW_11 = function(y, D, prop) BW(y, D, prop, nu=11),
    BW_81 = function(y, D, prop) BW(y, D, prop, nu=81),
    ELW=ELW
)
```

```{r label = "DR Estimators Instantiation"}
#' Imputation model: Linear
DR_Estimators.linear = list(
    AIPW=AIPW,
    AIPW_Stablized=AIPW_Stablized,
    WLS=WLS,
    Pi_Cov_1=Pi_Cov_1,
    Pi_Cov_2=Pi_Cov_2,
    TMLE=TMLE,
    DR_BR = function(y, D, x, prop_model_formula, impute_model_formula) DR_BR(y, D, x, prop_model_formula, impute_model_formula, type="linear"),
    ELW_DR=ELW_DR,
    DR_Bounded_IPW=DR_Bounded_IPW,
    DR_Bounded_Regression=DR_Bounded_Regression,
    CBPS=CBPS,
    CTD=CTD
    # * We temporarily remove ZZ estimators as they are too slow.
    # ZZ_13 = function(y, D, prop, y_imputed) ZZ(y, D, prop, y_imputed, hn_power=-1/3),
    # ZZ_15 = function(y, D, prop, y_imputed) ZZ(y, D, prop, y_imputed, hn_power=-1/5)
)


#' Imputation model: Logistic
DR_Estimators.logistic = list(
    AIPW,
    AIPW_Stablized,
    TMLE,
    DR_BR = function(y, D, x, prop_model_formula, impute_model_formula) DR_BR(y, D, x, prop_model_formula, impute_model_formula, type="logistic"),
    ELW_DR,
    DR_Bounded_IPW,
    CBPS
    # ZZ_13 = function(y, D, prop, y_imputed) ZZ(y, D, prop, y_imputed, hn_power=-1/3),
    # ZZ_15 = function(y, D, prop, y_imputed) ZZ(y, D, prop, y_imputed, hn_power=-1/5)
)
```


## True Mean


```{r label = "True Mean: Generating Data", cached = TRUE}
# define Generate data in the largest size to calculate the true population mean
data_true.linear = list(
    demystifing1 = data.gen1(N_large),  # linear imputation
    demystifing2 = data.gen2(N_large),  # linear imputation
    framework1 = data.gen3(N_large),  # linear imputation
    framework2 = data.gen4(N_large),  # linear imputation
    framework3 = data.gen5(N_large),  # linear imputation
    stable_eff = data.gen6(N_large),  # linear Imputation
    br = data.gen7(N_large)  # linear imputation
)

data_true.logistic = list(
    improved_DR1 = data.gen8(N_large),  # logistic imputation
    improved_DR2 = data.gen9(N_large)   # logistic imputation
)
```

```{r label = "Histogram of Propensity Scores"}
prop_histogram_all = list(
    demystifing1 = plot_prop_histogram(data_true.linear$demystifing1$prop, "Demystifying 1"),
    demystifing2 = plot_prop_histogram(data_true.linear$demystifing2$prop, "Demystifying 2"),
    framework1 = plot_prop_histogram(data_true.linear$framework1$prop, "Framework 1"),
    framework2 = plot_prop_histogram(data_true.linear$framework2$prop, "Framework 2"),
    framework3 = plot_prop_histogram(data_true.linear$framework3$prop, "Framework 3"),
    stable_eff = plot_prop_histogram(data_true.linear$stable_eff$prop, "Stable and Efficient"),
    br = plot_prop_histogram(data_true.linear$br$prop, "Bias-Reduced"),
    improved_DR1 = plot_prop_histogram(data_true.logistic$improved_DR1$prop, "Improved DR 1 (Demystifying 3)"),
    improved_DR2 = plot_prop_histogram(data_true.logistic$improved_DR2$prop, "Improved DR 2 (Demystifying 4)")
)
```

```{r label = "True Mean: Fitting Model"}
fitted_model_true.linear = list(
    demystifing1 = do.call(Fitted.model1$new, data_true.linear$demystifing1$data),
    demystifing2 = do.call(Fitted.model2$new, data_true.linear$demystifing2$data),
    framework1 = do.call(Fitted.model3$new, data_true.linear$framework1$data),
    framework2 = do.call(Fitted.model4$new, data_true.linear$framework2$data),
    framework3 = do.call(Fitted.model5$new, data_true.linear$framework3$data),
    stable_eff = do.call(Fitted.model6$new, data_true.linear$stable_eff$data),
    br = do.call(Fitted.model7$new, data_true.linear$br$data)
)

fitted_model_true.logistic = list(
    improved_DR1 = do.call(Fitted.model8$new, data_true.logistic$improved_DR1$data),
    improved_DR2 = do.call(Fitted.model9$new, data_true.logistic$improved_DR2$data)
)
```

```{r label = "True Mean: Obtaining Results"}
# no need to obtain parameters from fitted models

results_true.linear = list(
    demystifing1 = mean(fitted_model_true.linear$demystifing1$y),
    demystifing2 = mean(fitted_model_true.linear$demystifing2$y),
    framework1 = mean(fitted_model_true.linear$framework1$y),
    framework2 = mean(fitted_model_true.linear$framework2$y),
    framework3 = mean(fitted_model_true.linear$framework3$y),
    stable_eff = mean(fitted_model_true.linear$stable_eff$y),
    br = mean(fitted_model_true.linear$br$y)
)

results_true.logistic = list(
    improved_DR1 = mean(fitted_model_true.logistic$improved_DR1$y),
    improved_DR2 = mean(fitted_model_true.logistic$improved_DR2$y)
)

# todo make a one-row table

# No need to analyze them as well
```


## Both Models are Correct


```{r label = "Both Correct: Generating Data", cached = TRUE}
data_11.linear = pbmclapply(1:n_rep, function(i) {
    list(
        demystifing1 = data.gen1(N),
        demystifing2 = data.gen2(N),
        framework1 = data.gen3(N),
        framework2 = data.gen4(N),
        framework3 = data.gen5(N),
        stable_eff = data.gen6(N),
        br = data.gen7(N)
    )
}, mc.cores = num_cores)  # a list of n_rep elements, each element is also a list of lists (triple lists)

data_11.lgistic = pbmclapply(1:n_rep, function(i) {
    list(
        improved_DR1 = data.gen8(N),
        improved_DR2 = data.gen9(N)
    )
}, mc.cores = num_cores)
```

```{r label = "Both Correct: Fitting Model", cached = TRUE}
fitted_model_11.linear = pbmclapply(1:n_rep, function(i) {
    list(
        demystifing1 = do.call(Fitted.model1$new, data_11.linear[[i]]$demystifing1$data),
        demystifing2 = do.call(Fitted.model2$new, data_11.linear[[i]]$demystifing2$data),
        framework1 = do.call(Fitted.model3$new, data_11.linear[[i]]$framework1$data),
        framework2 = do.call(Fitted.model4$new, data_11.linear[[i]]$framework2$data),
        framework3 = do.call(Fitted.model5$new, data_11.linear[[i]]$framework3$data),
        stable_eff = do.call(Fitted.model6$new, data_11.linear[[i]]$stable_eff$data),
        br = do.call(Fitted.model7$new, data_11.linear[[i]]$br$data)
    )
}, mc.cores = num_cores)

fitted_model_11.logistic = pbmclapply(1:n_rep, function(i) {
    list(
        improved_DR1 = do.call(Fitted.model8$new, data_11.logistic[[i]]$improved_DR1$data),
        improved_DR2 = do.call(Fitted.model9$new, data_11.logistic[[i]]$improved_DR2$data)
    )
}, mc.cores = num_cores)
```

```{r label = "Both Correct: Obtaining Parameters", cached = TRUE}
params_11.linear = pbmclapply(1:n_rep, function(i) {
    list(
        demystifing1 = obtain_params_list(fitted_model_11.linear[[i]]$demystifing1, prop_model = TRUE, impute_model = TRUE),
        demystifing2 = obtain_params_list(fitted_model_11.linear[[i]]$demystifing2, prop_model = TRUE, impute_model = TRUE),
        framework1 = obtain_params_list(fitted_model_11.linear[[i]]$framework1, prop_model = TRUE, impute_model = TRUE),
        framework2 = obtain_params_list(fitted_model_11.linear[[i]]$framework2, prop_model = TRUE, impute_model = TRUE),
        framework3 = obtain_params_list(fitted_model_11.linear[[i]]$framework3, prop_model = TRUE, impute_model = TRUE, prop.specify = TRUE, prop.correct = TRUE),
        stable_eff = obtain_params_list(fitted_model_11.linear[[i]]$stable_eff, prop_model = TRUE, impute_model = TRUE),
        br = obtain_params_list(fitted_model_11.linear[[i]]$br, prop_model = TRUE, impute_model = TRUE)
    )
}, mc.cores = num_cores)

params_11.logistic = pbmclapply(1:n_rep, function(i) {
    list(
        improved_DR1 = obtain_params_list(fitted_model_11.logistic[[i]]$improved_DR1, prop_model = TRUE, impute_model = TRUE),
        improved_DR2 = obtain_params_list(fitted_model_11.logistic[[i]]$improved_DR2, prop_model = TRUE, impute_model = TRUE)
    )
}, mc.cores = num_cores)

save(data_11.linear, 
     data_11.logistic,
     fitted_model_11.linear,
     fitted_model_11.logistic,
     params_11.linear,
     params_11.logistic,
     file = "data/simulation-v2/11.RData")
```

```{r label = "Both Correct: Obtaining Results"}
# obtain_results(params_11.linear$demystifing1, estimators.linear)

# todo save in .Rdata

```

```{r label = "Both Correct: Analyzing Results"}

```


## Missing Model is Incorrect



## Imputation Model is Incorrect



## Both Models are incorrect


TODO: Make a nice plot comparing all parameters!


# Questions

1. Is it possible to include real data analysis such as Lalonde data for doubly robust case?
2. Since I know the data generating mechanism, I think we don't need Bootstrap to calculate standard error?
3. Two methods I don't know how to implement yet:
    + Rotnitzky, A., et al. “Improved Double-Robust Estimation in Missing Data and Causal Inference Models.” Biometrika, vol. 99, no. 2, June 2012, pp. 439–56. DOI.org (Crossref), https://doi.org/10.1093/biomet/ass013.
    + “Multiply Robust Imputation Procedures for the Treatment of Item Nonresponse in Surveys.” Biometrika, 2017. DOI.org (Crossref), https://doi.org/10.1093/biomet/asx007.
4. ZZ estimator is $O(N^2)$ and I don't know whether it can be further optimzied.

